{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELSR-torch\n",
    "Implementation of the paper [\"ELSR: Extreme Low-Power Super Resolution Network For Mobile Devices\"](https://arxiv.org/abs/2208.14600) using PyTorch. The code replicates the method proposed by the paper, but it is meant to be trained on limited devices. For that purpose the dataset is drastically smaller, and the training is way simpler.\n",
    "\n",
    "### Requirements\n",
    " - pytorch=1.13.1\n",
    " - opencv=4.7.0\n",
    " - pillow=9.4.0\n",
    " - matplotlib"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "The ELSR model is a small sub-pixel convolutional neural network with 6 layers. Only 5 of them have learnable parameters. The architecture is shown in the image below: (code in the following cell)\n",
    "\n",
    "![elsr](./plots/elsr.png \"Model architecture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.prelu = nn.PReLU()\n",
    "        self.conv2 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.prelu(out)\n",
    "        out = self.conv2(out)\n",
    "        return x + out\n",
    "\n",
    "\n",
    "class ELSR(nn.Module):\n",
    "    def __init__(self, upscale_factor):\n",
    "        super(ELSR, self).__init__()\n",
    "        self.layer1 = nn.Conv2d(3, 6, kernel_size=3, padding=1)\n",
    "        self.layer2_4 = ResBlock(6, 6)\n",
    "        self.layer5 = nn.Conv2d(6, 3 * (upscale_factor ** 2), kernel_size=3, padding=1)     # 6 -> 48\n",
    "        self.layer6 = nn.PixelShuffle(upscale_factor)\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                if m.out_channels == 48:\n",
    "                    nn.init.normal_(m.weight.data, mean=0.0, std=0.001)\n",
    "                    nn.init.zeros_(m.bias.data)\n",
    "                else:\n",
    "                    nn.init.normal_(m.weight.data, mean=0.0, std=math.sqrt(2/(m.out_channels*m.weight.data[0][0].numel())))\n",
    "                    nn.init.zeros_(m.bias.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2_4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = self.layer6(x)\n",
    "        return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PixelShuffle\n",
    "The PixelShuffle block (also known as depth2space) that performs computationally efficient upsampling by rearranging pixels in an image to increase its spatial resolution. Formally, let **x** be a tensor of size (**batch_size**, **C_in**, **H_in**, **W_in**), where **C_in** is the number of input channels, **H_in** and **W_in** are the height and width of the input, respectively. The goal of PixelShuffle is to upsample the spatial resolution of **x** by a factor of **r**, meaning that the output should be a tensor of size (**batch_size**, **C_out**, **H_in** * **r**, **W_in** * **r**), where **C_out** = **C_in** // **r^2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.modules.pixelshuffle.PixelShuffle'>\n"
     ]
    }
   ],
   "source": [
    "model = ELSR(upscale_factor=4)\n",
    "print(type(model.layer6))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "ELSR is trained on the REDS dataset, composed of sets of 300 videos, each set has a different degradation. My model is trained on a drastically reduced version of the dataset, containing only 30 videos with lower resolution (the original dataset was too big for me to train). The dataset (h5 files) is available at the following link: [https://drive.google.com/drive/folders/158bbeXr6EtCiuLI5wSh3SYRWMaWxK0Mq?usp=sharing](https://drive.google.com/drive/folders/158bbeXr6EtCiuLI5wSh3SYRWMaWxK0Mq?usp=sharing). The Dataset classes were defined in this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, h5_file):\n",
    "        super(TrainDataset, self).__init__()\n",
    "        self.h5_file = h5_file\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.h5_file, 'r') as f:\n",
    "            return f['lr'][idx] / 255., f['hr'][idx] / 255.\n",
    "\n",
    "    def __len__(self):\n",
    "        with h5py.File(self.h5_file, 'r') as f:\n",
    "            return len(f['lr'])\n",
    "\n",
    "class ValDataset(Dataset):\n",
    "    def __init__(self, h5_file):\n",
    "        super(ValDataset, self).__init__()\n",
    "        self.h5_file = h5_file\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.h5_file, 'r') as f:\n",
    "            return f['lr'][idx] / 255., f['hr'][idx] / 255.\n",
    "\n",
    "    def __len__(self):\n",
    "        with h5py.File(self.h5_file, 'r') as f:\n",
    "            return len(f['lr'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate my dataset I downscaled some videos from the REDS dataset using the following functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_training_data(data_path, out_path, scale):\n",
    "\n",
    "    if(not os.path.exists(out_path)):\n",
    "        os.makedirs(out_path)\n",
    "\n",
    "    c = 0\n",
    "    if len(os.listdir(out_path)) > 0: c += len(os.listdir(out_path))\n",
    "    for folder, j in zip(sorted(os.listdir(data_path)), range(10)):\n",
    "        for image in sorted(os.listdir(os.path.join(data_path, folder))):\n",
    "            image_path = os.path.join(data_path, folder, image)\n",
    "            resized_image = resize_image(image_path, scale)\n",
    "            plt.imsave(f'{out_path}{c}.png', resized_image)\n",
    "            c = c+1\n",
    "\n",
    "def generate_validation_set(data_path_X, out_path_X, data_path_Y, out_path_Y, scale):\n",
    "\n",
    "    if(not os.path.exists(out_path_X)):\n",
    "        os.makedirs(out_path_X)\n",
    "\n",
    "    c = 0\n",
    "    for folder, j in zip(sorted(os.listdir(data_path_X)), range(10)):\n",
    "        for image in sorted(os.listdir(os.path.join(data_path_X, folder))):\n",
    "            image_path = os.path.join(data_path_X, folder, image)\n",
    "            resized_image = resize_image(image_path, scale)\n",
    "            plt.imsave(f'{out_path_X}{c}.png', resized_image)\n",
    "            c = c+1\n",
    "\n",
    "    if(not os.path.exists(out_path_Y)):\n",
    "        os.makedirs(out_path_Y)\n",
    "\n",
    "    c = 0\n",
    "    for folder, j in zip(sorted(os.listdir(data_path_Y)), range(10)):\n",
    "        for image in sorted(os.listdir(os.path.join(data_path_Y, folder))):\n",
    "            image_path = os.path.join(data_path_Y, folder, image)\n",
    "            resized_image = resize_image(image_path, scale)\n",
    "            plt.imsave(f'{out_path_Y}{c}.png', resized_image)\n",
    "            c = c+1\n",
    "\n",
    "def resize_image(img_path, scale):\n",
    "    image = cv2.imread(img_path)\n",
    "    resized_image = cv2.resize(image, dsize=(image.shape[1]//scale, image.shape[0]//scale), interpolation=cv2.INTER_CUBIC)\n",
    "    resized_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB)\n",
    "    return resized_image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation\n",
    "To prevent overfitting and achieve better training results, I've augmented the video frames in my dataset using random augmentation between flipping, rotation and zoom, code below.\n",
    "\n",
    "**Notice the augmentation is the same for the (low_res, high_res) pairs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def augment_data(low_res, high_res):\n",
    "    # Read images\n",
    "    low_res = cv2.cvtColor(cv2.imread(low_res), cv2.COLOR_BGR2RGB)\n",
    "    high_res = cv2.cvtColor(cv2.imread(high_res), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Randomly choose a type of augmentation\n",
    "    aug_type = random.choice([\"flip\", \"rotate\", \"zoom\", \"none\"])\n",
    "\n",
    "    # Perform the chosen type of augmentation\n",
    "    if aug_type == \"flip\":\n",
    "        low_res = cv2.flip(low_res, 1)\n",
    "        high_res = cv2.flip(high_res, 1)\n",
    "    elif aug_type == \"rotate\":\n",
    "        angle = random.uniform(-30, 30)\n",
    "        rowsLR, colsLR = low_res.shape[:2]\n",
    "        MLR = cv2.getRotationMatrix2D((colsLR/2, rowsLR/2), angle, 1)\n",
    "        low_res = cv2.warpAffine(low_res, MLR, (colsLR, rowsLR))\n",
    "        rowsHR, colsHR = high_res.shape[:2]\n",
    "        MHR = cv2.getRotationMatrix2D((colsHR/2, rowsHR/2), angle, 1)\n",
    "        high_res = cv2.warpAffine(high_res, MHR, (colsHR, rowsHR))\n",
    "    elif aug_type == \"zoom\":\n",
    "        zoom_scale = random.uniform(0.8, 1.2)\n",
    "        rowsLR, colsLR = low_res.shape[:2]\n",
    "        MLR = cv2.getRotationMatrix2D((colsLR/2, rowsLR/2), 0, zoom_scale)\n",
    "        low_res = cv2.warpAffine(low_res, MLR, (colsLR, rowsLR))\n",
    "        rowsHR, colsHR = high_res.shape[:2]\n",
    "        MHR = cv2.getRotationMatrix2D((colsHR/2, rowsHR/2), 0, zoom_scale)\n",
    "        high_res = cv2.warpAffine(high_res, MHR, (colsHR, rowsHR))\n",
    "    \n",
    "    return low_res, high_res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "The training of the ELSR model is split in 6 steps in the paper, using different loss functions and different frame patch sizes. Nonetheless, for this implementation the images in the dataset are much smaller, hence only 3 steps are needed since we can use full-size images. Notice the number of epochs is reduced and the learning rate scheduler of the first training step is used even in the others. PSNR is used as a validation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def psnr(img1, img2):\n",
    "    return 10. * torch.log10(1. / torch.mean((img1 - img2) ** 2))\n",
    "\n",
    "def train(model, dataloader, loss_fn, optimizer, device, scheduler):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for i, data in enumerate(dataloader):\n",
    "        lr, hr = data\n",
    "        lr, hr = lr.to(device), hr.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        sr = model(lr)\n",
    "        np.save(\"plot_data/lr.npy\", lr[0].cpu().numpy().transpose(1,2,0))\n",
    "        np.save(\"plot_data/hr.npy\", hr[0].cpu().numpy().transpose(1,2,0))\n",
    "        np.save(\"plot_data/sr.npy\", sr[0].detach().cpu().numpy().transpose(1,2,0))\n",
    "        loss = loss_fn(sr, hr)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(dataloader)\n",
    "    return avg_train_loss\n",
    "\n",
    "def validate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    psnr_sum = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloader):\n",
    "            lr, hr = data\n",
    "            lr, hr = lr.to(device), hr.to(device)\n",
    "            sr = model(lr)\n",
    "            psnr_sum += psnr(sr, hr)\n",
    "\n",
    "    avg_psnr = psnr_sum / len(dataloader)\n",
    "    return avg_psnr\n",
    "\n",
    "\n",
    "# [...] See training.py for full implementation\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = ELSR(upscale_factor=args.scale).to(device)\n",
    "criterion = nn.MSELoss() if args.loss == 'mse' else nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "lambda1 = lambda epoch: args.lr*0.5 if epoch > args.epochs // 5 * 2 else args.lr\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer=optimizer, lr_lambda=lambda1)\n",
    "\n",
    "train_dataset = TrainDataset(args.train)\n",
    "train_dataloader = DataLoader(dataset=train_dataset,\n",
    "                                batch_size=args.batch_size,\n",
    "                                shuffle=True,\n",
    "                                pin_memory=True)\n",
    "\n",
    "val_dataset = ValDataset(args.val)\n",
    "val_dataloader = DataLoader(dataset=val_dataset,\n",
    "                            batch_size=args.batch_size,\n",
    "                            shuffle=False)\n",
    "\n",
    "best_psnr = 0.0\n",
    "train_losses = []\n",
    "psnrs = []\n",
    "\n",
    "for epoch in range(1, args.epochs+1):\n",
    "    train_loss = train(model=model, dataloader=train_dataloader, loss_fn=criterion, optimizer=optimizer, device=device, scheduler=scheduler)\n",
    "    val_psnr = validate(model=model, dataloader=val_dataloader, device=device)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    psnrs.append(val_psnr)\n",
    "\n",
    "    if val_psnr > best_psnr:\n",
    "        best_psnr = val_psnr\n",
    "        torch.save(model.state_dict(), os.path.join(args.out,f'best_X{args.scale}_model.pth'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training step 1\n",
    "Train the model on the x2 dataset using the L1 loss:\n",
    "```bash\n",
    "python training.py \\\n",
    "\t--train \"datasets/h5/train_X2.h5\" \\\n",
    "\t--val \"datasets/h5/val_X2.h5\" \\\n",
    "\t--out \"checkpoints/\" \\\n",
    "\t--scale 2 \\\n",
    "\t--epochs 300 \\\n",
    "\t--loss \"mae\" \\\n",
    "\t--lr 0.01\n",
    "```\n",
    "\n",
    "### Training step 2\n",
    "Fine-tune the pre-trained model from step 1 using the x4 dataset. Use L1 loss and use a higher learning rate. In the paper this is done in 2 steps, using different patch-sizes.\n",
    "```bash\n",
    "python training.py \\\n",
    "\t--train \"datasets/h5/train_X4.h5\" \\\n",
    "\t--val \"datasets/h5/val_X4.h5\" \\\n",
    "\t--out \"checkpoints/\" \\\n",
    "\t--scale 4 \\\n",
    "\t--epochs 50 \\\n",
    "\t--loss \"mae\" \\\n",
    "\t--lr 0.05 \\\n",
    "\t--weights \"best_X2_model.pth\"\n",
    "```\n",
    "\n",
    "### Training step 3\n",
    "Fine-tune the pre-trained model from step 2 using the x4 dataset. Use MSE loss and use a lower learning rate. In the paper this is done in 3 steps, using different patch-sizes.\n",
    "```bash\n",
    "python training.py \\\n",
    "\t--train \"datasets/h5/train_X4.h5\" \\\n",
    "\t--val \"datasets/h5/val_X4.h5\" \\\n",
    "\t--out \"checkpoints/\" \\\n",
    "\t--scale 4 \\\n",
    "\t--epochs 250 \\\n",
    "\t--loss \"mse\" \\\n",
    "\t--lr 5e-3 --weights \"best_X4_model.pth\"\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "Due to the used dataset I wasn't able to replicate the papers results, but indeed there are interesting results proving that video-super-resolution can be done with such a small model. The graphs below are the training losses through each training step:\n",
    "\n",
    "![](./plots/training_losses.png)\n",
    "\n",
    "### Tests\n",
    "\n",
    "The testing of single frame super-resolution is done in this way (video-sr is achieved by iterating sr on every frame):\n",
    " 1. Resize the input image to (image.height // upscale_factor, image.width // upscale_factor) using Bicubic interpolation\n",
    " 2. Calculate the bicubic_upsampled image of the previously produced low resolution image by the same upscaling factor using Bicubic interpolation\n",
    " 3. Use the low resolution image to predict the sr_image\n",
    " 4. Calculate PSNR between sr_image and bicubic_upsampled\n",
    "The results are shown below:\n",
    "\n",
    "![](./plots/sanremo_upscaled.png)\n",
    "\n",
    "The PSNR of the generated image has shown to be lower, but the resulting images are smoother, making bigger images better-looking:\n",
    "\n",
    "![](./plots/sonic_upscaled.png)\n",
    "\n",
    "Blurring stands out in pixelated images:\n",
    "\n",
    "![](./plots/pika_upscaled.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-time video super-resolution\n",
    "Of course tests on videos have been done. To achieve \"real-time\" video-sr the model should be able to preduct at least 30 FPS. Results are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSNR of Bicubic upscaled: 28.802486419677734 dB\n",
      "PSNR of Super-resoluted video: 28.447572708129883 dB\n",
      "FPS: 2632.0\n"
     ]
    }
   ],
   "source": [
    "from test_video import test_video\n",
    "from preprocessing import psnr\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "SCALE = 4\n",
    "WEIGHTS = \"./checkpoints/best_X4_model.pth\"\n",
    "INPUT = \"test/video/\"\n",
    "\n",
    "cudnn.benchmark = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = ELSR(upscale_factor=SCALE).to(device)\n",
    "\n",
    "state_dict = torch.load(WEIGHTS)\n",
    "model.load_state_dict(state_dict=state_dict)\n",
    "model.eval()\n",
    "\n",
    "video = []\n",
    "for frame_path in os.listdir(INPUT):\n",
    "    frame = cv2.cvtColor(cv2.imread(os.path.join(INPUT, frame_path)), cv2.COLOR_BGR2RGB)\n",
    "    video.append(frame)\n",
    "\n",
    "sr_video, bicubic_video, video, t = test_video(model, device, video, upscale_factor=SCALE)\n",
    "\n",
    "avg_psnr = 0.0\n",
    "for sr_img, image in zip(sr_video, video):\n",
    "    avg_psnr += psnr(sr_img, image)\n",
    "avg_psnr /= len(sr_video)\n",
    "\n",
    "bicubic_psnr = 0.0\n",
    "for bicubic_image, image in zip(bicubic_video, video): \n",
    "    bicubic_psnr += psnr(bicubic_image, image)\n",
    "bicubic_psnr /= len(bicubic_video)\n",
    "\n",
    "print(f\"PSNR of Bicubic upscaled: {bicubic_psnr} dB\")\n",
    "print(f\"PSNR of Super-resoluted video: {avg_psnr} dB\")\n",
    "print(f'FPS: {1/(t/len(sr_video)):.1f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Bicubic GIF: 28.80 dB  | ELSR GIF: 28.45 dB    |\n",
    "| ------------- | ------------- |\n",
    "| ![](./out/bicubic_video.gif)  | ![](./out/sr_video.gif)  |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "To me it's incredible that such a small model (17 KB) is able to outperform some bigger models just by using a CNN. Xiaomi researchers proved that GAN models, which are way too computationally expensive for mobile devices, can be easily replaced by something like this. I'm not disappointed in terms of results, because I believe that with a better dataset they would've been better. In particular, my \"ground truth\" images in the training set were replaced by video frames downscaled using Bicubuc interpolation, so I didn't expect the sr-output to get a higher PSNR than the Bicubic upsampled one's. Hence, I think it learnt to reproduce Bicubic interpolation with much less computational power, combined with some smoothing that comes from the convolutions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elsr-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fece98c1d8afb4f19757fb501ac1a6d8c8c8575fde52deaf52402ba9d0273f41"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
